{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BartForConditionalGeneration, BartTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model\n",
    "model_path = \"../resources/models/muss_en_mined_hf\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "base_model = BartForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1', '2': 'LABEL_2', '3': 'LABEL_3', '4': 'LABEL_4', '5': 'LABEL_5', '6': 'LABEL_6', '7': 'LABEL_7', '8': 'LABEL_8', '9': 'LABEL_9', '10': 'LABEL_10', '11': 'LABEL_11', '12': 'LABEL_12', '13': 'LABEL_13', '14': 'LABEL_14', '15': 'LABEL_15', '16': 'LABEL_16', '17': 'LABEL_17', '18': 'LABEL_18', '19': 'LABEL_19'}. The number of labels wil be overwritten to 20.\n",
      "Some weights of the model checkpoint at ../resources/models/classifiers/asset_test/muss_en_mined_hf-all-def-111111111000-000000000000/checkpoint-15780 were not used when initializing BartForConditionalGeneration: ['classification_head.out_proj.weight', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# base model\n",
    "model_path = \"../resources/models/classifiers/asset_test/muss_en_mined_hf-all-def-111111111000-000000000000/checkpoint-15780\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "cls_model = BartForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_auto_class', '_backward_compatibility_gradient_checkpointing', '_backward_hooks', '_buffers', '_call_impl', '_can_retrieve_inputs_from_name', '_convert_head_mask_to_5d', '_create_or_get_repo', '_expand_inputs_for_generation', '_forward_hooks', '_forward_pre_hooks', '_from_config', '_get_backward_hooks', '_get_decoder_start_token_id', '_get_logits_processor', '_get_logits_warper', '_get_name', '_get_repo_url_from_name', '_get_resized_embeddings', '_get_resized_lm_head', '_get_stopping_criteria', '_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_init_weights', '_is_full_backward_hook', '_keys_to_ignore_on_load_missing', '_keys_to_ignore_on_load_unexpected', '_keys_to_ignore_on_save', '_load_from_state_dict', '_load_pretrained_model', '_load_pretrained_model_low_mem', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_merge_criteria_processor_list', '_modules', '_named_members', '_no_split_modules', '_non_persistent_buffers_set', '_parameters', '_prepare_attention_mask_for_generation', '_prepare_decoder_input_ids_for_generation', '_prepare_encoder_decoder_kwargs_for_generation', '_prepare_input_ids_for_generation', '_prepare_model_inputs', '_push_to_hub', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_reorder_cache', '_replicate_for_data_parallel', '_resize_final_logits_bias', '_resize_token_embeddings', '_save_to_state_dict', '_set_default_torch_dtype', '_set_gradient_checkpointing', '_slow_forward', '_state_dict_hooks', '_tie_encoder_decoder_weights', '_tie_or_clone_weights', '_update_model_kwargs_for_generation', '_version', 'add_memory_hooks', 'add_module', 'adjust_logits_during_generation', 'apply', 'base_model', 'base_model_prefix', 'beam_sample', 'beam_search', 'bfloat16', 'buffers', 'children', 'compute_transition_beam_scores', 'config', 'config_class', 'constrained_beam_search', 'cpu', 'create_extended_attention_mask_for_decoder', 'cuda', 'device', 'double', 'dtype', 'dummy_inputs', 'dump_patches', 'estimate_tokens', 'eval', 'extra_repr', 'final_logits_bias', 'float', 'floating_point_ops', 'forward', 'framework', 'from_pretrained', 'generate', 'get_buffer', 'get_decoder', 'get_encoder', 'get_extended_attention_mask', 'get_extra_state', 'get_head_mask', 'get_input_embeddings', 'get_output_embeddings', 'get_parameter', 'get_position_embeddings', 'get_submodule', 'gradient_checkpointing_disable', 'gradient_checkpointing_enable', 'greedy_search', 'group_beam_search', 'half', 'init_weights', 'invert_attention_mask', 'is_gradient_checkpointing', 'is_parallelizable', 'lm_head', 'load_state_dict', 'main_input_name', 'model', 'modules', 'name_or_path', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_parameters', 'parameters', 'post_init', 'prepare_decoder_input_ids_from_labels', 'prepare_inputs_for_generation', 'prune_heads', 'push_to_hub', 'register_backward_hook', 'register_buffer', 'register_for_auto_class', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_module', 'register_parameter', 'requires_grad_', 'reset_memory_hooks_state', 'resize_position_embeddings', 'resize_token_embeddings', 'retrieve_modules_from_names', 'sample', 'save_pretrained', 'set_extra_state', 'set_input_embeddings', 'set_output_embeddings', 'share_memory', 'state_dict', 'supports_gradient_checkpointing', 'tie_weights', 'to', 'to_empty', 'train', 'training', 'type', 'warnings_issued', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "print(dir(base_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (bparam, cparam) in enumerate(zip(base_model.parameters(), cls_model.parameters())):\n",
    "# #     if i < 2:\n",
    "# #         print(param)\n",
    "# #         print(dir(param))\n",
    "# #         print(param.name)\n",
    "# #         print(bparam, cparam)\n",
    "#     params_are_equal = bparam.eq(cparam)\n",
    "# #         print(params_are_equal.shape)\n",
    "#     if not torch.all(params_are_equal):\n",
    "#         print(bparam)\n",
    "                                     \n",
    "for i, (bparam, cparam) in enumerate(zip(base_model.named_parameters(), cls_model.named_parameters())):\n",
    "    if bparam[0] == cparam[0]:\n",
    "        params_are_equal = bparam[-1].eq(cparam[-1])\n",
    "        if not torch.all(params_are_equal):\n",
    "            print(f'{bparam[0]} and {cparam[0]} do not match')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctrl_tokens",
   "language": "python",
   "name": "ctrl_tokens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
