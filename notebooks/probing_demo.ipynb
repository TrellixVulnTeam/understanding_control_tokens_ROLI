{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BartForConditionalGeneration, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<class 'torch.cuda.device'>\n",
      "1\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "GPU = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    print(torch.cuda.current_device())\n",
    "    print(torch.cuda.device)\n",
    "    torch.cuda.device = GPU\n",
    "    print(torch.cuda.device)\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../resources/models/muss_en_mined_hf\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path, output_attentions=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = read_lines('../resources/data/examples.en')\n",
    "\n",
    "def construct_dataset(sentences: List[str], ctrl_token: str = 'len_ratio', step_size: float = 0.25):\n",
    "\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for sent in sentences:\n",
    "        # possible combinations = num_trials^4\n",
    "        \n",
    "        for a in np.arange(0.25, 1.0, step_size):\n",
    "            a = round(a, 2) # target values = 0.0, 0.05, 0.1, 0.15, ...\n",
    "\n",
    "            if ctrl_token == 'len_ratio':\n",
    "                params = {\n",
    "                    'len_ratio': a,\n",
    "                    'lev_sim': 1.0,\n",
    "                    'word_rank': 1.0,\n",
    "                    'tree_depth': 1.0,\n",
    "                }\n",
    "            elif ctrl_token == 'lev_sim':\n",
    "                params = {\n",
    "                    'len_ratio': 1.0,\n",
    "                    'lev_sim': a,\n",
    "                    'word_rank': 1.0,\n",
    "                    'tree_depth': 1.0,\n",
    "                }\n",
    "            elif ctrl_token == 'word_rank':\n",
    "                params = {\n",
    "                    'len_ratio': 1.0,\n",
    "                    'lev_sim': 1.0,\n",
    "                    'word_rank': a,\n",
    "                    'tree_depth': 1.0,\n",
    "                }\n",
    "            elif ctrl_token == 'tree_depth':\n",
    "                params = {\n",
    "                    'len_ratio': 1.0,\n",
    "                    'lev_sim': 1.0,\n",
    "                    'word_rank': 1.0,\n",
    "                    'tree_depth': a,\n",
    "                }\n",
    "            \n",
    "            inputs.append(construct_input_for_access(sent, params))\n",
    "            labels.append(a)      \n",
    "\n",
    "    assert len(inputs) == len(labels)\n",
    "    \n",
    "    print(f'Constructed {len(inputs)} inputs')\n",
    "    \n",
    "    return inputs, labels\n",
    "\n",
    "        \n",
    "def get_hidden_states(sentences, model, tokenizer, batch_size: int = 12):\n",
    "    \n",
    "    for indx in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[indx:min(indx + batch_size, len(sentences))]\n",
    "        print(len(batch_sentences))\n",
    "        batch = tokenizer(batch_sentences, padding='max_length', return_tensors=\"pt\").to(model.device)\n",
    "        encoder_outputs = model.get_encoder()(batch['input_ids'], return_dict=True, output_hidden_states=True, output_attentions=True)\n",
    "        yield encoder_outputs.last_hidden_state\n",
    "\n",
    "def average_states(batch_states: torch.Tensor) -> torch.Tensor:\n",
    "    '''\n",
    "    creates a feature matrix with one row per sentence by averaging the tokens per sentence.\n",
    "    states: batch_size * max_seq_len (1024) * hidden_dim (1024)\n",
    "\n",
    "    Returns:\n",
    "        X           the averaged feature matrix\n",
    "    '''\n",
    "    print(batch_states.shape)\n",
    "    print(batch_states[0].shape)\n",
    "    \n",
    "    X = batch_states.mean(1)\n",
    "    print(X.shape)\n",
    "\n",
    "    return X.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# def save_to_file(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed 18 inputs\n"
     ]
    }
   ],
   "source": [
    "raw_inputs, raw_labels = construct_dataset(sentences)\n",
    "# print(raw_inputs, raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n",
      "1\n",
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "for hidden_states in get_hidden_states(raw_inputs, model, tokenizer, 1):\n",
    "    x = average_states(hidden_states)\n",
    "    x_train.append(x)\n",
    "\n",
    "x_train = np.stack(x_train)\n",
    "#     print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.18944564, -0.11129951, -0.10309957, ..., -0.04129167,\n",
       "          0.080787  , -0.17564222]],\n",
       "\n",
       "       [[ 0.18934718, -0.11219722, -0.1028005 , ..., -0.0404486 ,\n",
       "          0.0813223 , -0.17640522]],\n",
       "\n",
       "       [[ 0.18907292, -0.11207882, -0.10220982, ..., -0.04000543,\n",
       "          0.08089276, -0.17623329]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.1883909 , -0.11119529, -0.09323843, ..., -0.05045968,\n",
       "          0.1017652 , -0.16385254]],\n",
       "\n",
       "       [[ 0.1885542 , -0.11209796, -0.09294678, ..., -0.04987385,\n",
       "          0.1023041 , -0.16487736]],\n",
       "\n",
       "       [[ 0.18803838, -0.11190365, -0.0923159 , ..., -0.04913919,\n",
       "          0.10175003, -0.16446602]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "labeller = LabelEncoder()\n",
    "y = labeller.fit_transform(raw_labels)\n",
    "\n",
    "#view transformed values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x_train, y = shuffle(x_train, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.18944564, -0.11129951, -0.10309957, ..., -0.04129167,\n",
       "          0.080787  , -0.17564222]], dtype=float32),\n",
       " array([[ 0.18934718, -0.11219722, -0.1028005 , ..., -0.0404486 ,\n",
       "          0.0813223 , -0.17640522]], dtype=float32),\n",
       " array([[ 0.19518192, -0.11354618, -0.09207138, ..., -0.04744828,\n",
       "          0.08278838, -0.17461583]], dtype=float32),\n",
       " array([[ 0.19659495, -0.11638469, -0.10364174, ..., -0.04273542,\n",
       "          0.09063621, -0.17947604]], dtype=float32),\n",
       " array([[ 0.19707559, -0.11565908, -0.1046043 , ..., -0.0439654 ,\n",
       "          0.09055544, -0.17893913]], dtype=float32),\n",
       " array([[ 0.20172504, -0.11770041, -0.09758388, ..., -0.05647797,\n",
       "          0.09202515, -0.17125711]], dtype=float32),\n",
       " array([[ 0.1885542 , -0.11209796, -0.09294678, ..., -0.04987385,\n",
       "          0.1023041 , -0.16487736]], dtype=float32),\n",
       " array([[ 0.1883909 , -0.11119529, -0.09323843, ..., -0.05045968,\n",
       "          0.1017652 , -0.16385254]], dtype=float32),\n",
       " array([[ 0.18799928, -0.11443926, -0.10565773, ..., -0.04486449,\n",
       "          0.10087506, -0.1749631 ]], dtype=float32),\n",
       " array([[ 0.18907292, -0.11207882, -0.10220982, ..., -0.04000543,\n",
       "          0.08089276, -0.17623329]], dtype=float32),\n",
       " array([[ 0.18831652, -0.11378324, -0.10657376, ..., -0.04621951,\n",
       "          0.10072939, -0.17432186]], dtype=float32),\n",
       " array([[ 0.18803838, -0.11190365, -0.0923159 , ..., -0.04913919,\n",
       "          0.10175003, -0.16446602]], dtype=float32),\n",
       " array([[ 0.19679801, -0.11670035, -0.10419486, ..., -0.04312427,\n",
       "          0.09084152, -0.1796512 ]], dtype=float32),\n",
       " array([[ 0.20156953, -0.11681202, -0.09775853, ..., -0.05710023,\n",
       "          0.09143171, -0.1703959 ]], dtype=float32),\n",
       " array([[ 0.19568014, -0.11370724, -0.09276974, ..., -0.04812799,\n",
       "          0.08306323, -0.17498758]], dtype=float32),\n",
       " array([[ 0.18847418, -0.11486796, -0.10632268, ..., -0.04552852,\n",
       "          0.10131384, -0.17537262]], dtype=float32),\n",
       " array([[ 0.201312  , -0.11746483, -0.09691219, ..., -0.05586629,\n",
       "          0.09167064, -0.17093153]], dtype=float32),\n",
       " array([[ 0.19554904, -0.11292646, -0.0929746 , ..., -0.04867354,\n",
       "          0.08270386, -0.17402914]], dtype=float32)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting LogisticRegression classifier on len_ratio token...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. LogisticRegression expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             pickle\u001b[38;5;241m.\u001b[39mdump(clf, outfile)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clf\n\u001b[0;32m---> 39\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_linear_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlen_ratio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mtrain_linear_model\u001b[0;34m(X_train, y_train, probe_token, out_dir)\u001b[0m\n\u001b[1;32m     27\u001b[0m clf_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(clf)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfitting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m classifier on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobe_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m token...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     Path(out_dir)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/ctrl_tokens/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1138\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m-> 1138\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1146\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[0;32m~/.conda/envs/ctrl_tokens/lib/python3.8/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.conda/envs/ctrl_tokens/lib/python3.8/site-packages/sklearn/utils/validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[0;32m-> 1074\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/.conda/envs/ctrl_tokens/lib/python3.8/site-packages/sklearn/utils/validation.py:893\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    890\u001b[0m     )\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m    899\u001b[0m     _assert_all_finite(\n\u001b[1;32m    900\u001b[0m         array,\n\u001b[1;32m    901\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    902\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    903\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    904\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. LogisticRegression expected <= 2."
     ]
    }
   ],
   "source": [
    "def train_linear_model(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    probe_token: str,\n",
    "    out_dir: str = None,\n",
    "    ):\n",
    "    '''\n",
    "    Fits classifiers for a probing task experiment.\n",
    "    Args:\n",
    "        X_train     training features\n",
    "        y_train     training labels\n",
    "        probe_token     name of the ctrl token used for naming files\n",
    "        out_dir     output directory\n",
    "    '''\n",
    "    linear_clf_params = {'C': 0.0001,\n",
    "                         'max_iter': 100,\n",
    "#                          'solver': 'liblinear',\n",
    "                         'solver': 'lbfgs',\n",
    "#                          multi_class='multinomial', solver=''\n",
    "                         'tol': 1e-4,\n",
    "                         'verbose':100,\n",
    "                         'multi_class': 'multinomial',\n",
    "                         }\n",
    "    \n",
    "    clf = LogisticRegression(**linear_clf_params)\n",
    "\n",
    "    clf_name = str(clf).split('(')[0]\n",
    "    print(f'fitting {clf_name} classifier on {probe_token} token...')\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    if out_dir is not None:\n",
    "        Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "        out_path = Path(out_dir) / f'clf_{clf_name}_{probe_token}.pkl'\n",
    "        with open(str(out_path), 'wb') as outfile:\n",
    "            pickle.dump(clf, outfile)\n",
    "\n",
    "    return clf\n",
    "\n",
    "clf = train_linear_model(x_train, y, 'len_ratio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_mlp_model(\n",
    "#     X_train: np.ndarray,\n",
    "#     y_train: np.ndarray,\n",
    "#     probe_token: str,\n",
    "#     out_dir: str\n",
    "#     ):\n",
    "    \n",
    "#     mlp_clf_params = {'activation': 'relu',\n",
    "#                   'alpha': 0.0001,\n",
    "#                   'beta_1': 0.9,\n",
    "#                   'epsilon': 10e-8,\n",
    "#                   'hidden_layer_sizes': (100,),\n",
    "#                   'learning_rate_init': 0.0001,\n",
    "#                   'solver': 'adam',\n",
    "#                   'early_stopping':True,\n",
    "#                   'n_iter_no_change':10,\n",
    "#                   'validation_fraction':0.02,\n",
    "#                   'verbose':True\n",
    "#                   }\n",
    "\n",
    "#     mlp_clf = MLPClassifier(**mlp_clf_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3])\n",
      "tensor([[1.5000, 4.0000, 5.0000]])\n",
      "tensor([[[1., 4., 5.]],\n",
      "\n",
      "        [[2., 4., 5.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 4., 5., 2., 4., 5.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b = torch.tensor([[1.0, 4.0], [2.0, 4.0]])\n",
    "# print(b.shape)\n",
    "# print(b.mean(0))\n",
    "# print(b)\n",
    "# torch.reshape(b, (-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(source_matrix: torch.Tensor, bt_matrix: torch.Tensor) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    '''\n",
    "    creates an X matrix and a y vector for sklearn training from two tensors.\n",
    "    source tensor receives label 0, bt tensor receives label 1.\n",
    "    Args:\n",
    "        source_matrix   a torch.Tensor, the feature matrix for genuine text\n",
    "        bt_matrix       a torch.Tensor, the feature matrix for bt\n",
    "    '''\n",
    "    # creating DataFrames from the tensors and adding labels\n",
    "    source_df = pd.DataFrame(source_matrix.cpu().numpy())\n",
    "    source_df['label'] = [0] * len(source_df)\n",
    "    bt_df = pd.DataFrame(bt_matrix.cpu().numpy())\n",
    "    bt_df['label'] = [1] * len(bt_df)\n",
    "    \n",
    "    # combining and shuffling data\n",
    "    combined = source_df.append(bt_df, ignore_index=True)\n",
    "    combined = combined.sample(frac=1)\n",
    "\n",
    "    X = combined.drop(columns=['label'])\n",
    "    y = combined['label'].copy()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# def train_models(X_train: pd.DataFrame,\n",
    "#                  y_train: pd.DataFrame,\n",
    "#                  bt_name: str,\n",
    "#                  experiment: str,\n",
    "#                  out_dir: str,\n",
    "#                  logger: logging.Logger):\n",
    "#     '''\n",
    "#     Fits classifiers for a probing task experiment.\n",
    "#     Args:\n",
    "#         X_train     training features\n",
    "#         y_train     training labels\n",
    "#         bt_name     name of the bt dataset used for naming files\n",
    "#         experiment  the name of the experiment\n",
    "#         out_dir     output directory\n",
    "#         logger      a logging.Logger instance\n",
    "#     '''\n",
    "#     linear_clf_params = {'C': 0.0001,\n",
    "#                          'max_iter': 100,\n",
    "#                          'solver': 'liblinear',\n",
    "#                          'tol': 1e-4,\n",
    "#                          'verbose':100\n",
    "#                          }\n",
    "#     mlp_clf_params = {'activation': 'relu',\n",
    "#                       'alpha': 0.0001,\n",
    "#                       'beta_1': 0.9,\n",
    "#                       'epsilon': 10e-8,\n",
    "#                       'hidden_layer_sizes': (100,),\n",
    "#                       'learning_rate_init': 0.0001,\n",
    "#                       'solver': 'adam',\n",
    "#                       'early_stopping':True,\n",
    "#                       'n_iter_no_change':10,\n",
    "#                       'validation_fraction':0.02,\n",
    "#                       'verbose':True\n",
    "#                       }\n",
    "#     linear_clf = LogisticRegression(**linear_clf_params)\n",
    "#     mlp_clf = MLPClassifier(**mlp_clf_params)\n",
    "\n",
    "#     for clf in [linear_clf, mlp_clf]:\n",
    "#         clf_name = str(clf).split('(')[0]\n",
    "#         logger.info(f'fitting {clf_name} classifier on {bt_name} with {experiment}...')\n",
    "#         clf.fit(X_train, y_train)\n",
    "#         with open(os.path.join(out_dir, f'clf_{experiment}_{bt_name}_{clf_name}_fitted.pkl'), 'wb') as outfile:\n",
    "#             pickle.dump(clf, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args: argparse.Namespace):\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s | %(levelname)s | %(name)s | %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger = logging.getLogger('train_models.py')\n",
    "\n",
    "    out_dir = os.path.abspath(args.out_dir)\n",
    "\n",
    "    max_len = get_max_seq_len([args.genuine, args.bt], logger)\n",
    "\n",
    "    # training for padding experiment\n",
    "    padded_genuine_train = pad_states(os.path.join(args.genuine), max_len, logger)\n",
    "    padded_bt_train = pad_states(os.path.join(args.bt), max_len, logger)\n",
    "\n",
    "    padded_X_train, padded_y_train = create_dataset(padded_genuine_train, padded_bt_train)\n",
    "\n",
    "    train_models(padded_X_train,\n",
    "                 padded_y_train,\n",
    "                 args.bt_name,\n",
    "                 'padding',\n",
    "                 out_dir,\n",
    "                 logger)\n",
    "\n",
    "    # training for averaging experiment\n",
    "    averaged_genuine_train = average_states(os.path.join(args.genuine), logger)\n",
    "    averaged_bt_train = average_states(os.path.join(args.bt), logger)\n",
    "\n",
    "    averaged_X_train, averaged_y_train = create_dataset(averaged_genuine_train, averaged_bt_train)\n",
    "\n",
    "    train_models(averaged_X_train,\n",
    "                 averaged_y_train,\n",
    "                 args.bt_name,\n",
    "                 'averaging',\n",
    "                 out_dir,\n",
    "                 logger)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctrl_tokens",
   "language": "python",
   "name": "ctrl_tokens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
